<!doctype html>
<html>
  <head>
    <title>Test List Conversion</title>
  </head>
  <body>
    <!-- Test content similar to the Leonie Monigatti blog post -->
    <article>
      <h1>37 Things I Learned About Information Retrieval</h1>

      <ol>
        <li>
          BM25 is a strong baseline for search. Ha! You thought I would start with something about vector search, and
          here I am talking about keyword search.
        </li>
        <li>
          Vector search in vector databases is approximate and not exact. In theory, you could run a brute-force search
          to compute distances between a query vector and every vector in the database.
        </li>
        <li>
          Vector databases don't only store embeddings. They also store the original object (e.g., the text from which
          you generated the vector embeddings) and metadata.
        </li>
        <li>
          Vector databases' main application is not in generative AI. It's in search. But finding relevant context for
          LLMs is 'search'.
        </li>
        <li>
          You have to specify how many results you want to retrieve. When I think back, I almost have to laugh because
          this was such a big "aha" moment.
        </li>
      </ol>

      <!-- Alternative format that might be used -->
      <p>
        6. There are many different types of embeddings. When you think of a vector embedding, you probably visualize
        something like [-0.9837, 0.1044, 0.0090, â€¦, -0.2049].
      </p>
      <p>
        7. Fantastic embedding models and where to find them. The first place to go is the Massive Text Embedding
        Benchmark (MTEB).
      </p>
      <p>
        8. The majority of embedding models on MTEB are English. If you're working with multilingual or non-English
        languages, it might be worth checking out MMTEB.
      </p>
    </article>
  </body>
</html>
