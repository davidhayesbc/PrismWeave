// Generated by Copilot
// ONNX Runtime integration with graceful fallback handling

import * as vscode from 'vscode';
import { Logger } from '../utils/logger';

export interface ONNXModelProvider {
  isAvailable(): boolean;
  loadModel(modelPath: string): Promise<any>;
  runInference(session: any, inputs: any): Promise<any>;
  dispose(session: any): void;
}

/**
 * ONNX Runtime provider with graceful fallback
 * Handles optional dependency gracefully for cross-platform compatibility
 */
export class ONNXRuntimeProvider implements ONNXModelProvider {
  private _ort: any = null;
  private _isAvailable: boolean = false;

  constructor() {
    this._initializeORT();
  }

  private async _initializeORT(): Promise<void> {
    try {
      // Try to dynamically import onnxruntime-node
      this._ort = await import('onnxruntime-node');
      this._isAvailable = true;
      Logger.info('ONNX Runtime loaded successfully');
    } catch (error) {
      Logger.warn('ONNX Runtime not available:', error);
      this._isAvailable = false;
      
      // Show user-friendly message
      vscode.window.showWarningMessage(
        'ONNX Runtime not available. Local AI models will not work. ' +
        'This is expected in development environments.',
        'Learn More'
      ).then(selection => {
        if (selection === 'Learn More') {
          vscode.env.openExternal(vscode.Uri.parse(
            'https://github.com/microsoft/onnxruntime/tree/main/js/node'
          ));
        }
      });
    }
  }

  isAvailable(): boolean {
    return this._isAvailable && this._ort !== null;
  }

  async loadModel(modelPath: string): Promise<any> {
    if (!this.isAvailable()) {
      throw new Error('ONNX Runtime not available');
    }

    try {
      const session = await this._ort.InferenceSession.create(modelPath);
      Logger.info(`Model loaded successfully: ${modelPath}`);
      return session;
    } catch (error) {
      Logger.error('Failed to load ONNX model:', error);
      throw new Error(`Failed to load model: ${error}`);
    }
  }

  async runInference(session: any, inputs: any): Promise<any> {
    if (!this.isAvailable()) {
      throw new Error('ONNX Runtime not available');
    }

    try {
      const results = await session.run(inputs);
      return results;
    } catch (error) {
      Logger.error('ONNX inference failed:', error);
      throw new Error(`Inference failed: ${error}`);
    }
  }

  dispose(session: any): void {
    if (session && typeof session.release === 'function') {
      try {
        session.release();
        Logger.debug('ONNX session disposed');
      } catch (error) {
        Logger.warn('Error disposing ONNX session:', error);
      }
    }
  }

  getProviderInfo(): string {
    if (!this.isAvailable()) {
      return 'ONNX Runtime: Not Available';
    }

    try {
      // Try different ways to get available execution providers
      // API varies between versions
      let providers: string[] = ['CPU']; // Default fallback
      
      if (this._ort.InferenceSession.getAvailableProviders) {
        providers = this._ort.InferenceSession.getAvailableProviders();
      } else if (this._ort.getAvailableProviders) {
        providers = this._ort.getAvailableProviders();
      } else {
        // Try to detect common providers
        const commonProviders = ['CPUExecutionProvider', 'CUDAExecutionProvider', 'DirectMLExecutionProvider'];
        providers = commonProviders.filter(provider => {
          try {
            // This is a heuristic - we can't easily test provider availability without creating a session
            return true;
          } catch {
            return false;
          }
        });
        
        if (providers.length === 0) {
          providers = ['CPU']; // Ultimate fallback
        }
      }
      
      return `ONNX Runtime: Available (Providers: ${providers.join(', ')})`;
    } catch (error) {
      return `ONNX Runtime: Available (Version: ${this._ort.version || 'Unknown'})`;
    }
  }
}

/**
 * Mock provider for when ONNX Runtime is not available
 * Provides stub implementations for testing and fallback
 */
export class MockONNXProvider implements ONNXModelProvider {
  constructor() {
    // No initialization needed for mock
  }

  isAvailable(): boolean {
    return false;
  }

  async loadModel(modelPath: string): Promise<any> {
    Logger.warn(`Mock ONNX: Attempted to load model ${modelPath}`);
    throw new Error('ONNX Runtime not available - using mock provider');
  }

  async runInference(session: any, inputs: any): Promise<any> {
    Logger.warn('Mock ONNX: Attempted inference');
    throw new Error('ONNX Runtime not available - using mock provider');
  }

  dispose(session: any): void {
    Logger.debug('Mock ONNX: Dispose called');
  }
}

/**
 * Factory function to create the appropriate ONNX provider
 */
export function createONNXProvider(): ONNXModelProvider {
  // Always try the real provider first, it will handle fallback internally
  return new ONNXRuntimeProvider();
}

/**
 * AI Model abstraction that works with or without ONNX Runtime
 */
export class AIModelManager {
  private _provider: ONNXModelProvider;
  private _loadedModels: Map<string, any> = new Map();

  constructor() {
    this._provider = createONNXProvider();
  }

  get isONNXAvailable(): boolean {
    return this._provider.isAvailable();
  }

  get providerInfo(): string {
    if (this._provider instanceof ONNXRuntimeProvider) {
      return this._provider.getProviderInfo();
    }
    return 'Mock ONNX Provider';
  }

  async loadModel(modelId: string, modelPath: string): Promise<void> {
    if (!this._provider.isAvailable()) {
      Logger.warn(`Cannot load model ${modelId}: ONNX Runtime not available`);
      return;
    }

    try {
      const session = await this._provider.loadModel(modelPath);
      this._loadedModels.set(modelId, session);
      Logger.info(`Model ${modelId} loaded successfully`);
    } catch (error) {
      Logger.error(`Failed to load model ${modelId}:`, error);
      throw error;
    }
  }

  async runInference(modelId: string, inputs: any): Promise<any> {
    const session = this._loadedModels.get(modelId);
    if (!session) {
      throw new Error(`Model ${modelId} not loaded`);
    }

    return await this._provider.runInference(session, inputs);
  }

  unloadModel(modelId: string): void {
    const session = this._loadedModels.get(modelId);
    if (session) {
      this._provider.dispose(session);
      this._loadedModels.delete(modelId);
      Logger.info(`Model ${modelId} unloaded`);
    }
  }

  unloadAllModels(): void {
    for (const [modelId, session] of this._loadedModels) {
      this._provider.dispose(session);
      Logger.debug(`Unloaded model: ${modelId}`);
    }
    this._loadedModels.clear();
  }

  dispose(): void {
    this.unloadAllModels();
  }
}
